{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa208ee",
   "metadata": {},
   "source": [
    "BASELINE MODELS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "metrics = pd.read_csv(\"../reports/metrics_noq/metrics_summary.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Keep only columns you care to show\n",
    "cols = [\n",
    "    \"model\",\n",
    "    \"test_accuracy\",\n",
    "    \"test_precision\",\n",
    "    \"test_recall\",\n",
    "    \"test_f1\",\n",
    "    \"test_roc_auc\",\n",
    "]\n",
    "tab_baseline = metrics[cols].copy()\n",
    "\n",
    "# Optional: round for prettier LaTeX\n",
    "tab_baseline = tab_baseline.round(3)\n",
    "\n",
    "Path(\"tables\").mkdir(exist_ok=True)\n",
    "tab_baseline.to_latex(\n",
    "    \"tables/baseline_models_noq.tex\",\n",
    "    index=False,\n",
    "    caption=\"Performance of baseline classifiers on the held-out test set.\",\n",
    "    label=\"tab:baseline_models_noq\",\n",
    "    escape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc413c",
   "metadata": {},
   "source": [
    "Tuned models table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12eaff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned = pd.read_csv(\"../reports/hparam_tuning_noq/tuned_models_summary.tsv\", sep=\"\\t\")\n",
    "\n",
    "cols_tuned = [\n",
    "    \"model\",\n",
    "    \"test_accuracy\",\n",
    "    \"test_precision\",\n",
    "    \"test_recall\",\n",
    "    \"test_f1\",\n",
    "    \"test_roc_auc\",\n",
    "]\n",
    "tab_tuned = tuned[cols_tuned].copy().round(3)\n",
    "\n",
    "Path(\"tables\").mkdir(exist_ok=True)\n",
    "tab_tuned.to_latex(\n",
    "    \"tables/tuned_models_noq.tex\",\n",
    "    index=False,\n",
    "    caption=\"Performance of tuned classifiers on the held-out test set.\",\n",
    "    label=\"tab:tuned_models_noq\",\n",
    "    escape=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cde74",
   "metadata": {},
   "source": [
    "Baseline F1 bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21d1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"figures\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e97103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_sorted = metrics.sort_values(\"test_f1\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.barh(metrics_sorted[\"model\"], metrics_sorted[\"test_f1\"])\n",
    "plt.xlabel(\"Test F1\")\n",
    "plt.title(\"Baseline models\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/baseline_f1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bbf32",
   "metadata": {},
   "source": [
    "Before vs after tuning (F1 + AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b0ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "Path(\"figures\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635472fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "baseline_path = Path(\"../reports/metrics_noq/metrics_summary.tsv\")\n",
    "tuned_path    = Path(\"../reports/hparam_tuning_noq/tuned_models_summary.tsv\")\n",
    "\n",
    "baseline = pd.read_csv(baseline_path, sep=\"\\t\")\n",
    "tuned    = pd.read_csv(tuned_path,    sep=\"\\t\")\n",
    "\n",
    "# Clean up old column if present\n",
    "for df in (baseline, tuned):\n",
    "    if \"model_base\" in df.columns:\n",
    "        del df[\"model_base\"]\n",
    "\n",
    "# Normalised key\n",
    "tuned[\"model_base\"]    = tuned[\"model\"].str.replace(\"_tuned$\", \"\", regex=True)\n",
    "baseline[\"model_base\"] = baseline[\"model\"]\n",
    "\n",
    "# Drop original 'model' to avoid suffix clash\n",
    "for df in (baseline, tuned):\n",
    "    if \"model\" in df.columns:\n",
    "        del df[\"model\"]\n",
    "\n",
    "# Merge\n",
    "merged = baseline.merge(\n",
    "    tuned,\n",
    "    on=\"model_base\",\n",
    "    suffixes=(\"_base\", \"_tuned\"),\n",
    ")\n",
    "\n",
    "# Nice display name and sort by tuned F1\n",
    "merged[\"model_name\"] = merged[\"model_base\"]\n",
    "merged = merged.sort_values(\"test_f1_tuned\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# This is what the plotting code expects\n",
    "plot_df = merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e44429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/r5b1vnb91cjffx5_zt6rs8v80000gn/T/ipykernel_93698/350248566.py:5: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[0].set_xticklabels(plot_df[\"model_name\"], rotation=45, ha=\"right\")\n",
      "/var/folders/sj/r5b1vnb91cjffx5_zt6rs8v80000gn/T/ipykernel_93698/350248566.py:12: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[1].set_xticklabels(plot_df[\"model_name\"], rotation=45, ha=\"right\")\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=False)\n",
    "\n",
    "axes[0].bar(plot_df[\"model_name\"], plot_df[\"test_f1_base\"], label=\"baseline\")\n",
    "axes[0].bar(plot_df[\"model_name\"], plot_df[\"test_f1_tuned\"], alpha=0.7, label=\"tuned\")\n",
    "axes[0].set_xticklabels(plot_df[\"model_name\"], rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"Test F1\")\n",
    "axes[0].set_title(\"F1 before vs after tuning\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].bar(plot_df[\"model_name\"], plot_df[\"test_roc_auc_base\"], label=\"baseline\")\n",
    "axes[1].bar(plot_df[\"model_name\"], plot_df[\"test_roc_auc_tuned\"], alpha=0.7, label=\"tuned\")\n",
    "axes[1].set_xticklabels(plot_df[\"model_name\"], rotation=45, ha=\"right\")\n",
    "axes[1].set_ylabel(\"Test ROC–AUC\")\n",
    "axes[1].set_title(\"ROC–AUC before vs after tuning\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/f1_auc_tuning.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845daa71",
   "metadata": {},
   "source": [
    "ROC + PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d036ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "\n",
    "from mitochime.hyperparam_search_top import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ddad4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load test set (no-quality)\n",
    "X_test, y_test, feature_names = load_dataset(\"../data/processed/test_noq.tsv\")\n",
    "\n",
    "# 2) Load tuned models you want to analyse\n",
    "models = {\n",
    "    \"catboost\":           load(\"../models_noq_tuned/catboost_tuned.joblib\"),\n",
    "    \"gradient_boosting\":  load(\"../models_noq_tuned/gradient_boosting_tuned.joblib\"),\n",
    "    \"random_forest\":      load(\"../models_noq_tuned/random_forest_tuned.joblib\"),\n",
    "    \"logreg_l2\":          load(\"../models_noq_tuned/logreg_l2_tuned.joblib\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a61aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# ROC\n",
    "plt.subplot(1,2,1)\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "plt.plot([0,1], [0,1], \"k--\", lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curves\")\n",
    "plt.legend()\n",
    "\n",
    "# PR\n",
    "plt.subplot(1,2,2)\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    prec, rec, _ = precision_recall_curve(y_test, scores)\n",
    "    ap = average_precision_score(y_test, scores)\n",
    "    plt.plot(rec, prec, label=f\"{name} (AP={ap:.3f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall curves\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/roc_pr_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78258fb",
   "metadata": {},
   "source": [
    "Confusion matrices for 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f16d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"clean\", \"chimeric\"],\n",
    "                yticklabels=[\"clean\", \"chimeric\"])\n",
    "    plt.title(f\"Confusion matrix – {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/cm_{name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitochime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
