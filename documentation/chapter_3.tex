%   Filename    : chapter_3.tex
% \section*{Chapter 3}\label{sec:researchmethod}
\chapter{Research Methodology}

This chapter outlines the steps involved in completing the study, including data gathering, generating simulated mitochondrial Illumina reads, preprocessing and indexing the data, developing a bioinformatics pipeline to extract key features, applying machine learning algorithms for chimera detection, and validating and comparing model performance.

\section{Research Activities}

As illustrated in \figref{fig:process_diagram}, this study carried out a sequence of procedures to detect PCR-induced chimeric reads in mitochondrial genomes. The process began with collecting a mitochondrial reference sequence of \textit{Sardinella lemuru} from the National Center for Biotechnology Information (NCBI) database, which was used as a reference for generating simulated clean and chimeric reads. These reads were subsequently indexed and mapped. These datasets will go through a bioinformatics pipeline that includes k-mer extraction and homology-based filtering to prepare the data for model construction. The machine learning model will subsequently be trained and tested using the processed datasets to assess its precision and accuracy. The model will undergo refinement and retraining until it meets the required performance threshold, after which it will proceed to validation, testing, and deployment.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{figures/research_activities.png}
  \caption{Process Diagram of Special Project}\label{fig:process_diagram}
\end{figure}

\subsection{Data Collection}

The mitochondrial genome reference sequence of \textit{S.~lemuru} was obtained from the NCBI database (accession number NC\_039553.1) in FASTA format. This sequence served as the basis for generating simulated chimeric reads for model development.

This step is scheduled to begin in the first week of November~2025 and is expected to be completed by the last week of November~2025, with a total duration of approximately one (1) month.

\subsubsection{Data Preprocessing}

To reduce manual repetition, all steps in the simulation and preprocessing pipeline were executed using a custom script in Python (Version 3.11). The script runs each stage, including read simulation, reference indexing, mapping, and alignment processing, in a fixed sequence.

Sequencing data were simulated from the NCBI reference genome using \texttt{wgsim} (Version 1.13). First, 10{,}000 paired-end reads (R1 and R2) were generated from the original reference (\texttt{original\_reference.fasta}) and designated as clean reads using the command:

\begin{verbatim}
wgsim -1 150 -2 150 -r 0 -R 0 -X 0 -e 0.001 -N 10000 \
       original_reference.fasta ref1.fastq ref2.fastq
\end{verbatim}

The command parameters are as follows:
\begin{itemize}
    \item \texttt{-1} and \texttt{-2}: read lengths of 150~base pairs for each paired-end read.
    \item \texttt{-r}, \texttt{-R}, \texttt{-X}: mutation rate, fraction of indels, and indel extension probability, all set to a default value of 0.
    \item \texttt{-e}: base error rate, set to 0.001 to simulate realistic sequencing errors.
    \item \texttt{-N}: number of read pairs, set to 10{,}000.
\end{itemize}

Chimeric sequences were then generated from the same NCBI reference using a separate Python script. Two non-adjacent segments were randomly selected such that their midpoint distances fell within specified minimum and maximum thresholds. The script attempts to retain microhomology, or short identical sequences at segment junctions, to mimic PCR-induced template switching. The resulting chimeras were written to \texttt{chimera\_reference.fasta}, with headers recording segment positions and microhomology length.  
The \texttt{chimera\_reference.fasta} file was subsequently processed with \texttt{wgsim} to simulate 10{,}000 paired-end chimeric reads (\texttt{chimeric1.fastq} and \texttt{chimeric2.fastq}) using the same command format.

A custom script will be created to merge all simulated reads into a single dataset and assign class labels: clean reads as ``0'' and chimeric reads as ``1''. The dataset will then be partitioned so that 80\% are used for training the machine learning model and 20\% for testing. The merged and labeled dataset will be saved in TSV (\texttt{.tsv}) format. This will result in a balanced dataset with an equal number of clean and chimeric reads, which is important to prevent model bias and allow the machine learning classifiers to correctly distinguish chimeric reads.

Next, a \texttt{minimap2} index of the reference genome was created using:
\begin{verbatim}
minimap2 -d ref.mmi original_reference.fasta
\end{verbatim}

Minimap2 (Version 2.28) is a tool used to map reads to a reference genome. Mapping allows extraction of alignment features from each read, which will be used as input for the machine learning model. The simulated clean and chimeric reads were then mapped to the reference index (\texttt{ref.mmi}) as follows:
\begin{verbatim}
minimap2 -ax sr -t 8 ref.mmi ref1.fastq ref2.fastq > clean.sam
\end{verbatim}

\begin{verbatim}
minimap2 -ax sr -t 8 ref.mmi \
chimeric1.fastq chimeric2.fastq > chimeric.sam
\end{verbatim}

Here, \texttt{-ax sr} specifies short-read alignment mode, and \texttt{-t 8} uses 8 CPU threads. The resulting clean and chimeric SAM files contain the alignment positions of each read relative to the original reference genome.

The SAM files were then converted to BAM format, sorted, and indexed using \texttt{samtools} (Version 1.20):

\begin{verbatim}
samtools view -bS clean.sam -o clean.bam
samtools view -bS chimeric.sam -o chimeric.bam

samtools sort clean.bam -o clean.sorted.bam
samtools index clean.sorted.bam

samtools sort chimeric.bam -o chimeric.sorted.bam
samtools index chimeric.sorted.bam
\end{verbatim}

BAM files are the compressed binary version of SAM files, enabling faster processing and reduced storage. Sorting will arrange reads by genomic coordinates, and indexing will allow detection of supplementary alignments (SA) as a feature for the machine learning model.

This whole process is scheduled to start in the first week of November~2025 and is expected to be completed by the last week of November~2025, with a total duration of approximately one (1) month.

\subsection{Bioinformatics Tools Pipeline}

A bioinformatics pipeline will be developed and implemented to extract the necessary analytical features. This pipeline will serve as a reproducible and modular workflow that accepts FASTQ and BAM inputs, processes these through a series of analytical stages, and outputs tabular feature matrices (TSV) for downstream machine learning. All scripts will be version-controlled through GitHub, and computational environments will be standardized using Conda to ensure cross-platform reproducibility. To promote transparency and replicability, the exact software versions, parameters, and command-line arguments used in each stage will be documented. To further ensure correctness and adherence to best practices, bioinformatics experts at the Philippine Genome Center Visayas will be consulted to validate the pipeline design, feature extraction logic, and overall data integrity. This stage of the study is scheduled to begin in the last week of November~2025 and conclude by the last week of January~2026, with an estimated total duration of approximately two (2) months.

The bioinformatics pipeline focuses on three principal features from the simulated and aligned sequencing data: (1) supplementary alignment count (SA count), (2) k-mer composition difference between read segments, and (3) micro-homology length at potential junctions. Each of these features captures a distinct biological or computational signature associated with PCR-induced chimeras.

\subsubsection{Alignment and Supplementary Alignment Count}
This will be derived through sequence alignment using Minimap2, with subsequent processing performed using SAMtools and \texttt{pysam} in Python. Sequencing reads will be aligned to the \textit{Sardinella lemuru} mitochondrial reference genome using Minimap2 with the \texttt{-ax sr} preset (optimized for short reads). The output will be converted and sorted using SAMtools, producing an indexed BAM file which will be parsed using \texttt{pysam} to count the number of supplementary alignments (SA tags) per read. Each read’s mapping quality, number of split segments, and alignment characteristics will be recorded in a corresponding TSV file. The presence of multiple alignment loci within a single read, as reflected by a nonzero SA count, serves as direct computational evidence of chimerism. Reads that contain supplementary alignments or soft-clipped regions are strong candidates for chimeric artifacts arising from PCR template switching or improper assembly during sequencing.

\subsubsection{K-mer Composition Difference}
Chimeric reads often comprise fragments from distinct genomic regions, resulting in a compositional discontinuity between segments. Comparing k-mer frequency profiles between the left and right halves of a read allows detection of such abrupt compositional shifts, independent of alignment information. This will be obtained using Jellyfish, a fast k-mer counting software. For each read, the sequence will be divided into two segments, either at the midpoint or at empirically determined breakpoints inferred from supplementary alignment data, to generate left and right sequence segments. Jellyfish will then compute k-mer frequency profiles (with $k = 5$ or $6$) for each segment. The resulting k-mer frequency vectors will be normalized and compared using distance metrics such as cosine similarity or Jensen–Shannon divergence to quantify compositional disparity between the two halves of the same read. The resulting difference scores will be stored in a structured TSV file.

\subsubsection{Micro-homology Length}
The micro-homology length will be computed using a custom Python script that detects the longest exact suffix–prefix overlap within $\pm 30$ base pairs surrounding a candidate breakpoint. This analysis identifies the number of consecutive bases shared between the end of one segment and the beginning of another. The presence and length of such micro-homology are classic molecular signatures of PCR-induced template switching, where short identical regions (typically 3–15 base pairs) promote premature termination and recombination of DNA synthesis on a different template strand. Quantifying micro-homology allows assessment of whether the suspected breakpoint reflects PCR artifacts or true biological variants. Each read will therefore be annotated with its corresponding micro-homology length, overlap sequence, and GC content.

After extracting the three primary features, all resulting TSV files will be joined using the read identifier as a common key to generate a unified feature matrix. Additional read-level metadata such as read length, mean base quality, and number of clipped bases will also be included to provide contextual information. This consolidated dataset will serve as the input for subsequent machine-learning model development and evaluation.

\subsection{Machine Learning Model Development}

This study will explore multiple machine-learning approaches to detect PCR-induced chimeras from mitochondrial Illumina reads: Support Vector Machines (SVM) to separate reads with complex patterns, decision trees to capture hierarchical interactions among SA count, k-mer composition, and micro-homology length, logistic regression as a linear baseline, Random Forest (RF) to improve stability and reduce variance, and gradient boosting (e.g., XGBoost) to model non-linear relationships among the extracted features. Using these approaches enables a balanced assessment of predictive performance and interpretability.

The dataset will be divided into training (80\%) and testing (20\%) subsets. The training data will be used for model fitting and hyperparameter optimization through five-fold cross-validation, in which the data are partitioned into five folds; four folds are used for training and one for validation in each iteration. Performance metrics will be averaged across folds, and the optimal parameters will be selected based on mean cross-validation accuracy. The final models will then be evaluated on the held-out test set to obtain unbiased performance estimates.

Model development and evaluation will be implemented in Python (v3.11) using the \texttt{scikit-learn} and \texttt{xgboost} libraries. Standard metrics including accuracy, precision, recall, F1-score, and area under the ROC curve (AUC) will be computed to quantify predictive performance. 

\subsection{Validation and Testing}

Validation will involve both internal and external evaluations. Internal validation will be achieved through five-fold cross-validation on the training data to verify model generalization and reduce variance due to random sampling. External validation will be achieved through testing on the 20\% hold-out dataset derived from the simulated reads, which will serve as an unbiased benchmark to evaluate how well the trained models generalize to unseen data. All feature extraction and preprocessing steps will be performed using the same bioinformatics pipeline to ensure consistency and comparability across validation stages.

Comparative evaluation across all candidate algorithms, including SVM, decision trees, logistic regression, Random Forest, gradient boosting, and others, will determine which models demonstrate the highest predictive performance and computational efficiency under identical data conditions. Their metrics will be compared to identify the which algorithms are most suitable for further refinement.

\subsection{Documentation}

Comprehensive documentation will be maintained throughout the study to ensure transparency and reproducibility. All stages of the research, including data gathering, preprocessing, feature extraction, model training, and validation, will be systematically recorded in a \texttt{.README} file in the GitHub repository. For each analytical step, the corresponding parameters, software versions, and command line scripts will be documented to enable exact replication of results.

The repository structure will follow standard research data management practices, with clear directories for datasets and scripts. Computational environments will be standardized using Conda, with an environment file (\texttt{environment.arm.yml}) specifying dependencies and package versions to maintain consistency across systems.

For manuscript preparation and supplementary materials, Overleaf (\LaTeX) will be used to produce publication-quality formatting and consistent referencing.

\section{Calendar of Activities}

Table~\ref{tab:project_timeline} presents the project timeline in the form of a Gantt chart, where each bullet point corresponds to approximately one week of planned activity.

\begin{table}[H]
  \centering
  \caption{Timetable of Activities}\label{tab:project_timeline}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
      \hline
      \textbf{Activities (2025)} & \textbf{Nov} & \textbf{Dec} & \textbf{Jan} & \textbf{Feb} & \textbf{Mar} & \textbf{Apr} & \textbf{May} \\
      \hline
      Data Collection and Simulation & $\bullet\bullet\bullet\bullet$ &  &  &  &  &  &  \\
      \hline
      Bioinformatics Tools Pipeline & $\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ &  &  &  &  \\
      \hline
      Machine Learning Development &  &  & $\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet$ &  \\
      \hline
      Testing and Validation &  &  &  &  &  & $\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ \\
      \hline
      Documentation & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ & $\bullet\bullet\bullet\bullet$ \\
      \hline
    \end{tabular}%
  }
\end{table}