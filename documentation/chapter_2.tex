% %   Filename    : chapter_2.tex 
% \section*{Chapter 2}
% \section{Review of Related Literature}
% This chapter discusses the features, capabilities, and limitations of existing research, algorithms, or software that are related/similar to the Special Problem.
% \subsection{Ontologies in Computer Science}
% This chapter  contains a review of research papers that:
% One of the ultimate goals of ontology as a philosophy is to provide a definitive, exhaustive classification of entities across all spheres of being. However, in the context of computer and information science, this goal has transformed into the pursuit of creating a single unified system that resolves the differences of terminologies and concepts used across diverse data and knowledge-based systems \cite{smith2012ontology}. In fact, in their study on ontologies and knowledge-base systems, \citeA{kharbat2008building} claimed that ontology has been an emerging computer science discipline for decades. They also concluded that ontologies formalize the semantics of a domain of knowledge by explicitly describing the elements that comprise the domain. This meant that ontologies consisted of concepts that describe the internal features or attributes of an entity, as well as properties that describe the relationships between these entities. 

% \subsubsection {Applications of Ontologies}
% The aforementioned properties of ontologies in Kharbat and El-Ghalayini’s study  meant that ontologies are capable of performing a broad range of tasks across diverse research areas. The tasks that are relevant to the study include: the integration of heterogeneous data sources to overcome semantic heterogeneities \cite{lacroix2003bioinformatics}; the creation of knowledge bases \cite{noy2001ontology}; deriving aspects of information systems at run time \cite{guarino1998formal}, and the construction of an ontology-based retrieval system that can assist end users in browsing and understanding domain concepts \cite{baker1999ontology}. Furthermore, \citeA{munir2018use} stated that, with the recent dramatic increase in the use of knowledge discovery applications, there is a growing complexity in terms of the database search requests that the end users are supposed to write to retrieve the information that they wanted. \citeA{munir2018use} stipulated that these difficulties are attributed to the need for the end users to have a good understanding of the complex structure of databases, and the semantic relationships that exist between different data within the database. It is through the use of ontologies for knowledge representation and interactive query generation that researchers were able to improve the interface between data and search requests, increasing the accuracy of the result sets to the user search requirements. Building upon these applications of ontologies, the study adopts a similar approach, creating an ontological knowledge base that consolidates, organizes, and classifies Panayanon  myths, legends,  and folk tales that also depicts the settings, character relationships, and themes that are embedded in these Panayanon stories.

% \subsection{Ontology Development}

% \subsubsection{Ontology Construction}
% \citeA{yadav2016development} further expounds on the core components that form an ontology. These components of ontologies include:  a set of concepts that can serve as nodes in the representation of an ontology; an optional set of properties related to the concepts, these properties can also be summarized as the values of the concepts; a set of relational properties that implies relationship between two or more concepts, often generating a hierarchical path from one concept to another; a hierarchy of concepts and a hierarchy of properties as a result of the relational properties linking one concept to another; a transitive property relation that expands and allows for logical inference on relationships between properties; i.e., if Property A is related to Property B, and Property B is related to Property C, then Property A will be necessarily related to property C; symmetry and inverse symmetry relations  among properties; domain values related to properties that define the level of properties within classes, indicating that concepts that share the same property values have the same domains; range values related to the properties which can either be an interval, a list of elements, or a character; and minimum and maximum cardinality for each concept-property pair that define how many properties are associated with a particular concept. These core components of ontologies will be applied in developing the ontology for this study.\\

% \citeA{yadav2016development} also listed the basic steps in constructing ontologies. According to their study, the first step in constructing ontologies is determining its scope. These include defining the structure of the ontology as well as the values that are associated with the ontology. Next, is the consideration of reusing ontologies. \citeA{yadav2016development} stated that it’s possible to re-use recent ontologies in defining the schema of the new ontology that is to be constructed. Third, is the enumeration of terms, where all terms must be clearly specified, together with the domain and range of the ontology. Fourth, is the definition of the taxonomy, where all terms are organized in a hierarchy. For example, if A is a subclass of B, then every instance of class A must be an instance of B. Fifth, is the definition of properties, which includes specifying the properties that link the classes while organizing them in a hierarchy. Next, is the definition of facets which is defined as the hierarchy of homogeneous terms that describe an aspect of the domain where each term in the hierarchy refers to a different concept \cite{giunchiglia2012facet}. For example, if a domain is space, then facets might refer to bodies of water, land formations, and administrative divisions. Finally, the last step of ontology construction is the definition of instances within the ontology. The steps outlined by \citeA{yadav2016development} will be applied in constructing the ontology for this study. This includes the reuse of an existing ontology, building upon it by incorporating additional concepts, classes, and all of the other aforementioned core components  to expand the ontology’s scope and application.\\

% The construction of the ontology will be done through Protege,  an open-source knowledge requisition system written in Java \cite{yadav2016development, jain2013ontology}. More specifically, it's an ontology development editor that is capable of defining ontological concepts or classes, properties, taxonomies, and class instances. Protege supports ontology representation languages like OWL. Aside from constructing ontologies, \citeA{zhao2012research} states that Protege is also capable of parsing an Ontology model using a Protege-based OWL API. Protege is able to: load an ontology model from the OWL file; collect the classes, subclasses, object properties, data properties; and find the domain and range relevant to a particular object property. The study will be using Protege Desktop v.5.6.4 in developing the ontological database for the Panayanon stories. 

% \subsubsection{SPARQL for Ontology Querying}
% SPARQL 1.1 is a set of specifications that provide languages and protocols to query and manipulate RDF graph content on the Web or in an RDF store. The standard SPARQL Query Results are written in an XML Format, and in three other alternative formats: JSON, CSV, and TSV \cite{picalausa2011real}. SPARQL 1.1 is the query language the Protege uses to retrieve, and manipulate ontological data.

% \subsubsection{ApacheJena for Ontology Storage}

% According to the \citeA{apache-community-development-project-no-date}, ApacheJena is able to provide a complete framework for building Semantic Web and Linked Data applications in Java. ApacheJena is also equipped with the following capabilities: parsers for Turtle, N-triples, and Resource Description Framework (RDF), and Extensible Markup Language(XML); an API for programming with Java; a complete implementation of the SPARQL query language for ontological querying; a rule-based inference engine for RDF Schema (RDFS) amd OWL entailments; a Triple Database (TDB) which is a non-SQL persistent triples store; a Semantic Database (SDB) which is a persistent triples store built upon a relational store, and Fuseki, an RDF server that uses web protocols. The Apache Software Foundation claims that ApacheJena complies with the relevant recommendations for RDF and related technologies from the World Wide web Consortium (W3C). 

% In a study conducted by \citeA{chokshi2022using}, they were able to construct a Job Search Ontology on Protégé, integrated the ApacheJena Fuseki Server with the ontology, and executed SPARQL queries on the ApacheJena Fuseki Server without using the Protégé tool. This study demonstrated that it is possible to construct a SPARQL endpoint with Apache Jena. ApacheJena will be mainly used for storing data about the study’s ontology.  An Apache Fuseki Server will publish the study’s ontology as a SPARQL endpoint, making it available for querying and data sharing over the internet. 

% \subsection{Natural Language Question to SPARQL Translation}

% \subsubsection{Natural Language Question (NLQ) Preprocessing}

% spaCy is an open-source library for advanced natural language processing (NLP) in Python. spaCy is designed to handle preprocessing tasks with high efficiency and speed. spaCy’s features and functionalities include: tokenization, lemmatization, part-of-speech (POS) tagging, and named entity recognition (Nawaz,
% 2023; SpaCy Documentation, n.d.). In the study, spaCy will be used to preprocess the NLQ through tokenization, and lemmatization. 

% \subsubsection{Entity and Relationship Extraction with Semantic Parsing}

% According to \citeA{nawaz-2023}, spaCy is capable of named entity recognition (NER) and dependency parsing. In the study, spaCy’s NER and dependency parsing will be used to extract entities like folk tale titles, names of researchers, character names, and even the relationships between entities. These will  be passed to the SPARQL query constructor to create a SPARQL query and retrieve information from the study’s ontology. 

% \subsubsection{Semantic Parsing with SBERT}

% Sentence Transformers or SBERT,  is a Python module used for accessing, using, and training text and image embedding models. It can be used to compute embeddings using Sentence Transformer models or to calculate similarity scores using Cross-Encoder models. SBERT’s features and functionalities include: semantic search, semantic textual similarity, and paraphrase mining. The Semantic Textual Similarity (STS) application aims to  produce embeddings for all texts involved and calculate the similarities between them. The text pairs with the highest similarity score are considered to be the most semantically similar (SentenceTransformers Documentation, n.d.). In the study, STS will be used to embed phrases in the NLQ and compare them with the embeddings of the ontology’s object and data property labels. STS will also be used to help resolve ambiguous queries where multiple relationships can potentially be extracted from the query. 


% \subsubsection{Query Construction/Generation}
% RDFLib is a pure Python package made for working with RDF. RDFLib’s features and functions include: parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, JSON-LD, HexTuples, RDFa and Microdata; Store implementations like memory stores, and remote SPARQL endpoints; Graph interface either to a single graph or to multiple named graphs; and SPARQL 1.1 implementation \cite{unknown-author-no-date-rdflib}. In the study, RDFLib will be used to dynamically generate SPARQL queries together with the extracted entities, and relationships of the NLQ. 

% \subsection{Chatbot Development}
% \subsubsection{RASA Framework}

% Rasa Open Source is a Python framework that enables teams to build chatbots, voice assistants, and other automated conversation systems by connecting to messaging channels and third party systems through a set of APIs \cite{unknown-author-2024-rasa}. 

% In a study conducted by \citeA{mishra2022natural}, they  created a closed domain ontology for a hostel system usingProtégé, which was then referenced by an AI-powered chatbot through RASA that was able to formalize natural language queries into SPARQL to query knowledge bases. More specifically, in the study they were able to design a natural language query formalization pipeline that had intent recognition to determine the type of the user’s natural language query, entity extraction, and query generation to translate the query’s intent and extracted entities into a SPARQL query.  The study by \citeA{mishra2022natural} has shown that it’s possible to incorporate a NLQ to SPARQL pipeline within the chatbot. In the study, RASA open source will be used to construct the chat-bot. 


