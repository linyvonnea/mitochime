% \section*{Chapter 4}\label{sec:results}
\chapter{Results and Discussion}
\section{Descriptive Analysis of Features}

This chapter presents the performance of the proposed feature set and machine learning models for detecting PCR-induced chimeric reads in simulated mitochondrial Illumina data. The behaviour of the main features is first described, followed by a comparison of baseline classifiers, an assessment of the effect of hyperparameter tuning, and an analysis of feature importance in terms of individual variables and feature families.

The final dataset contained 31{,}986 reads for training and 7{,}997 reads for testing, with classes balanced (approximately 4{,}000 clean and 4{,}000 chimeric reads in the test split).

\subsection{Exploratory Data Analysis} 
An exploratory data analysis (EDA) was conducted on the extracted feature matrix to characterize general patterns in the data and gain preliminary insight into which variables might meaningfully contribute to classification. Histograms of key features indicated that alignment-based variables showed clear class separation as chimeric reads have higher frequencies of split alignments and broader long-tailed distribution on soft-clipped regions (\texttt{softclip\_left} and \texttt{softclip\_right}). In contrast, sequence-based variables such a microhomology length and k-mer divergence displayed substantial overlap between classes, suggesting more limited discriminative value. The complete set of histograms is provided in Appendix~\ref{appendix:appendix_A}.

As shown in Figure \ref{fig:correlation_heatmap}, the feature correlation heatmap shows that alignment-derived features form a strongly correlated cluster, whereas sequence-derived features show weak correlations with both the alignment-based features and with one another. This heterogeneity indicates that no single feature family captures all relevant signal sources.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../notebooks/figures/correlation.png}
    \caption{Feature correlation heatmap showing relationships among alignment-derived and sequence-derived features.}
    \label{fig:correlation_heatmap}
\end{figure}

\section{Descriptive Analysis of Features}
\subsection{Summary Statistics Per Class}

Summary statistics were computed separately for clean reads (class~0) and chimeric reads (class~1) to characterize the distributional behavior of the features. For each feature, the mean, standard deviation, median, first and third quartiles (Q1, Q3), interquartile range (IQR), minimum, maximum, and sample size ($n$) were calculated.

Only a subset of the features is summarized in the main text to highlight key trends, and not all summary statistics columns are shown for brevity. The complete set of per-class summary statistics for all features is provided in Appendix~\ref{app:full_feature_statistics}.

\subsubsection{Alignment and Supplementary Alignment Features}

Features related to supplementary alignments show strong separation between classes. Chimeric reads exhibit supplementary alignments, as reflected by higher values of \texttt{has\_sa}, \texttt{sa\_count}, and \texttt{num\_segments}, whereas clean reads consistently show a single alignment segment with no supplementary mappings. This behavior is consistent with the expected structure of chimeric reads and indicates that alignment-based features are highly informative.

\subsubsection{Clipping-Based Features}

Clipping-related features, including \texttt{softclip\_left}, \texttt{softclip\_right}, and \texttt{total\_clipped\_bases}, display higher means and broader distributions in chimeric reads. Clean reads are dominated by zero or near-zero clipping, while chimeric reads exhibit increased clipping and greater variability, which reflects the presence of split alignments.

\subsubsection{K-mer Distribution Features}

K-mer–based features, such as \texttt{kmer\_js\_divergence} and \texttt{kmer\_cosine\_diff}, show only modest differences between clean and chimeric reads. Chimeric reads show slightly higher average divergence, but substantial overlap with clean reads means this feature alone cannot reliably distinguish the classes.

\subsubsection{Microhomology Features}

Microhomology-related features (\texttt{microhomology\_length} and \texttt{microhomology\_gc}) exhibit nearly identical summary statistics across both classes. The majority of reads in both classes contain short or zero-length microhomologies, resulting in minimal separation. This means that microhomology serves as a weak standalone indicator and is more appropriately treated as supporting evidence.

Overall, the summary statistics indicate that alignment-based and clipping-based features provide the strongest class separation, k-mer features contribute limited but complementary signal, and microhomology features exhibit minimal discriminative power on their own. These observations motivate the combined multi-feature approach used in subsequent modeling and evaluation.

\begin{table}[H]
\centering
\caption{Summary statistics of selected key features by class.}
\label{tab:key_feature_summary}
\begin{tabular}{llrrrr}
\toprule
Feature & Class & Mean & Std & Median & IQR \\
\midrule
has\_sa & chimeric & 0.406 & 0.491 & 0.0 & 1.0 \\
has\_sa & clean    & 0.000 & 0.000 & 0.0 & 0.0 \\

num\_segments & chimeric & 1.406 & 0.491 & 1.0 & 1.0 \\
num\_segments & clean    & 1.000 & 0.000 & 1.0 & 0.0 \\

softclip\_left & chimeric & 12.55 & 21.90 & 0.0 & 19.0 \\
softclip\_left & clean    & 0.23  & 1.54  & 0.0 & 0.0  \\

softclip\_right & chimeric & 12.90 & 22.12 & 0.0 & 19.0 \\
softclip\_right & clean    & 0.21  & 1.51  & 0.0 & 0.0  \\

total\_clipped\_bases & chimeric & 25.44 & 25.48 & 19.0 & 48.0 \\
total\_clipped\_bases & clean    & 0.44  & 2.16  & 0.0  & 0.0  \\

kmer\_js\_divergence & chimeric & 0.974 & 0.025 & 0.986 & 0.043 \\
kmer\_js\_divergence & clean    & 0.976 & 0.025 & 0.986 & 0.040 \\

kmer\_cosine\_diff & chimeric & 0.974 & 0.026 & 0.986 & 0.042 \\
kmer\_cosine\_diff & clean    & 0.976 & 0.025 & 0.986 & 0.041 \\

microhomology\_length & chimeric & 0.458 & 0.755 & 0.0 & 1.0 \\
microhomology\_length & clean    & 0.462 & 0.758 & 0.0 & 1.0 \\

microhomology\_gc  & chimeric & 0.172 & 0.361 & 0.0    & 0.0    \\
microhomology\_gc  & clean    & 0.172 & 0.361 & 0.0    & 0.0    \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Boxplots By Class}

Boxplots were generated for each feature, with the x-axis representing the class clean reads and chimeric reads and the y-axis representing the feature value. Figure~\ref{fig:key_feature_boxplots} presents a panel of selected key features, while boxplots for all numeric features are provided in Appendix~\ref{app:all_boxplots}.

For clipping-related features, chimeric reads exhibit higher medians and longer upper whiskers than clean reads, indicating increased variability and the presence of split alignments. 

Supplementary alignment features show that clean reads are largely zero, whereas chimeric reads display a wider distribution, reflecting frequent supplementary alignments. 

K-mer metrics show a slight upward shift for chimeric reads, but substantial overlap with clean reads indicates modest discriminative power. 

Microhomology features have nearly overlapping distributions for both classes, consistent with their low standalone predictive importance. 

\begin{figure}[H]
\centering

\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_softclip_left.png}
    \caption{Softclip left}
    \label{fig:box_softclip_left}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_softclip_right.png}
    \caption{Softclip right}
    \label{fig:box_softclip_right}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_has_sa.png}
    \caption{Has SA}
    \label{fig:box_has_sa}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_sa_count.png}
    \caption{SA count}
    \label{fig:box_sa_count}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_total_clipped_bases.png}
    \caption{Total clipped bases}
    \label{fig:box_total_clipped_bases}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{../notebooks/figures/boxplots/box_kmer_js_divergence.png}
    \caption{K-mer JS divergence}
    \label{fig:box_kmer_js_divergence}
\end{subfigure}

\caption{Boxplots of key features by class}
\label{fig:key_feature_boxplots}
\end{figure}

\section{Baseline Classification Performance}

Table~\ref{tab:baseline_models_noq} summarises the performance of eleven classifiers trained on the engineered feature set using five-fold cross-validation and evaluated on the held-out test set. All models were optimised using default hyperparameters, without dedicated tuning.

The dummy baseline, which always predicts the same class regardless of the input features, achieved an accuracy of 0.50 and test F1-score of 0.67. This reflects the balanced class distribution and provides a lower bound for meaningful performance.

Across other models, test F1-scores clustered in a narrow band between approximately 0.74 and 0.77 and ROC--AUC values between 0.82 and 0.84. Gradient boosting, CatBoost, LightGBM, XGBoost, bagging trees, random forest, and multilayer perceptron (MLP) all produced very similar scores, with CatBoost and gradient boosting slightly ahead (test F1 $\approx 0.77$, ROC--AUC $\approx 0.84$). Linear models (logistic regression and calibrated linear SVM) performed only marginally worse (test F1 $\approx 0.74$), while Gaussian Naive Bayes lagged behind with substantially lower F1 ($\approx 0.65$) despite very high precision for the chimeric class.

\input{tables/baseline_models_noq}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{figures/baseline_f1.png}
  \caption{Test F1 of all baseline classifiers, showing that no single model clearly dominates and several achieve comparable performance.}
  \label{fig:baseline_f1}
\end{figure}

\section{Effect of Hyperparameter Tuning}

To assess whether performance could be improved further, ten model families underwent randomised hyperparameter search. The tuned metrics are summarised in Table~\ref{tab:tuned_models_noq}. Overall, tuning yielded modest but consistent gains for tree-based ensembles and boosting methods, while leaving linear models essentially unchanged or slightly worse.

CatBoost, gradient boosting, LightGBM, XGBoost, random forest, bagging trees, and MLP all experienced small increases in test F1 (typically $\Delta \mathrm{F1} \approx 0.002$--0.009) and ROC--AUC (up to $\Delta \mathrm{AUC} \approx 0.008$). After tuning, CatBoost remained the best performer with test accuracy 0.80, precision 0.92, recall 0.66, F1-score 0.77, and ROC--AUC 0.84. Gradient boosting achieved almost identical performance (F1 0.77, AUC 0.84). Random forest and bagging trees also improved to F1 scores around 0.76 with AUC $\approx 0.84$.

\input{tables/tuned_models_noq}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/f1_auc_tuning.png}
  \caption{Comparison of test F1 (left) and ROC--AUC (right) for baseline and tuned models.}
  \label{fig:f1_auc_tuning}
\end{figure}

Because improvements are small and within cross-validation variability, tuning was interpreted as stabilising and slightly refining the models rather than completely altering their behaviour or their relative ranking.

\section{Detailed Evaluation of Representative Models}

For interpretability and diversity, four tuned models were selected for deeper analysis: CatBoost (best-performing boosted tree), scikit-learn gradient boosting (canonical gradient-boosting implementation), random forest (non-boosted ensemble baseline), and $L_2$-regularised logistic regression (linear baseline). All models were trained on the engineered feature set and evaluated on the same held-out test data.

\subsection{Confusion Matrices and Error Patterns}

Classification reports and confusion matrices for the four models reveal consistent patterns. CatBoost and gradient boosting both reached overall accuracy of approximately 0.80 with similar macro-averaged F1 scores ($\sim 0.80$). For CatBoost, precision and recall for clean reads were 0.73 and 0.95, respectively, while for chimeric reads they were 0.92 and 0.66 (F1 = 0.77). Gradient boosting showed nearly identical trade-offs.

Random forest attained slightly lower accuracy (0.80) and chimeric F1 (0.76), whereas logistic regression achieved the lowest accuracy among the four (0.79) and chimeric F1 (0.74), although it provided the highest chimeric precision (0.95) at the cost of lower recall (0.61).

Across all models, errors were asymmetric. False negatives (chimeric reads predicted as clean) were more frequent than false positives. For example, CatBoost misclassified 1{,}369 chimeric reads as clean but only 215 clean reads as chimeric. This pattern indicates that the models are conservative and prioritise avoiding false chimera calls at the expense of missing some true chimeras. Consultation with PGC Visayas indicated that this conservative behavior is generally acceptable, though further evaluation and testing will be required to assess its impact on downstream analyses.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.45\linewidth]{figures/cm_catboost.png}
  \includegraphics[width=0.45\linewidth]{figures/cm_gradient_boosting.png}\\[4pt]
  \includegraphics[width=0.45\linewidth]{figures/cm_random_forest.png}
  \includegraphics[width=0.45\linewidth]{figures/cm_logreg_l2.png}
  \caption{Confusion matrices for the four representative models on the held-out test set.}
  \label{fig:confusion_matrices}
\end{figure}

\subsection{ROC and Precision--Recall Curves}

Receiver operating characteristic (ROC) and precision--recall (PR) curves as shown in Figure~\ref{fig:roc_pr_curves} further support the similarity among the top models. The three tree-based ensembles (CatBoost, gradient boosting, random forest) achieved ROC--AUC values of approximately 0.84 and average precision (AP) around 0.88. Logistic regression performed slightly worse (AUC $\approx 0.82$, AP $\approx 0.87$) but still substantially better than the dummy baseline.

The PR curves show that precision remains above 0.9 across a broad range of recall values (up to roughly 0.5--0.6), after which precision gradually declines. This behaviour indicates that the models can assign very high confidence to a subset of chimeric reads, while more ambiguous reads can only be recovered by accepting lower precision.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/roc_pr_curves.png}
  \caption{ROC (left) and precision--recall (right) curves for the four representative models on the held-out test set.}
  \label{fig:roc_pr_curves}
\end{figure}

\section{Feature Importance}

\subsection{Permutation Importance of Individual Features}

To understand how each classifier made predictions, feature importance was quantified using permutation importance. This analysis was applied to four representative models: CatBoost, Gradient Boosting, Random Forest, and L$_2$-regularized Logistic Regression.

As shown in Figure~\ref{fig:all_models_imp}, the total number of clipped bases consistently provides a strong predictive signal, particularly in Random Forest, Gradient Boosting, and L$_2$-regularized Logistic Regression. CatBoost differs by assigning the highest importance to k-mer divergence metrics such as \texttt{kmer\_js\_divergence}, which capture subtle sequence changes resulting from structural variants or PCR-induced chimeras. Soft-clipping features (\texttt{softclip\_left} and \texttt{softclip\_right}) provide more information around breakpoints, complementing these primary signals in all models except Gradient Boosting. L$_2$-regularized Logistic Regression relies more on alignment-based split-read metrics.

Overall, these results indicate that accurate detection of chimeric reads relies on both alignment-based signals and k-mer compositional information. Explicit microhomology features contribute minimally in this analysis, and combining both alignment-based and sequence-level features enhances model sensitivity and specificity.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/catboost_imp.png}
        \caption{CatBoost}
        \label{fig:catboost_imp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gradient_imp.png}
        \caption{Gradient Boosting}
        \label{fig:gradient_imp}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/randomforest_imp.png}
        \caption{Random Forest}
        \label{fig:randomforest_imp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/logreg_imp.png}
        \caption{L$_2$-regularized Logistic Regression}
        \label{fig:logreg_imp}
    \end{subfigure}

    \caption{Permutation-based feature importance for four representative classifiers.}
    \label{fig:all_models_imp}
\end{figure}

\subsection{Feature Family Importance}

To evaluate the contribution of broader signals, features were grouped into five families: 
SA\_structure (supplementary alignment and segment metrics, e.g., 
\texttt{has\_sa}, \texttt{sa\_count}, \texttt{sa\_min\_delta\_pos}, \texttt{sa\_mean\_nm}, etc.), 
Clipping (\texttt{softclip\_left}, \texttt{softclip\_right}, \texttt{total\_clipped\_bases}, 
\texttt{breakpoint\_read\_pos}), 
Kmer\_jump (\texttt{kmer\_cosine\_diff}, \texttt{kmer\_js\_divergence}), 
Micro\_homology (
\texttt{microhomology\_length}, \texttt{microhomology\_gc}), 
and Other (e.g., \texttt{mapq}).

Aggregated analyses reveal consistent patterns across models. In CatBoost, the Clipping family has the largest cumulative contribution (0.14), followed by Kmer\_jump (0.12), with Other features contributing minimally (0.005) and SA\_structure (0.003) and Micro\_homology (0.003) providing minimal predictive power. Gradient Boosting shows a similar trend, with Clipping (0.13) dominating, Kmer\_jump (0.11) secondary, and the remaining families contributing negligibly. Random Forest integrates both Clipping (0.088) and Kmer\_jump (0.08) effectively, while SA\_structure, Micro\_homology, and Other remain minor contributors. L$_2$-regularized Logistic Regression emphasizes Clipping (0.09) and SA\_structure (0.07), with Kmer\_jump and Micro\_homology having minimal impact.

Both feature-level and aggregated analyses indicate that detection of chimeric reads in this dataset relies primarily on alignment irregularities (Clipping) and k-mer compositional shifts (Kmer\_jump), which often arise from PCR-induced template switching events, while explicit microhomology features contribute minimally.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{catboost_family.png}
        \caption{CatBoost}
        \label{fig:catboost_family}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gradient_family.png}
        \caption{Gradient Boosting}
        \label{fig:gb_family}
    \end{subfigure}
    
    \vspace{0.5em}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{randomforest_family.png}
        \caption{Random Forest}
        \label{fig:rf_family}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{logreg_family.png}
        \caption{L$_2$-regularized Logistic Regression}
        \label{fig:logreg_family}
    \end{subfigure}

    \caption{Aggregated feature family importance across four models.}
    \label{fig:feature_family_all}
\end{figure}

\section{Feature Selection}

Feature selection was performed to identify the smallest subset reaching 95\% cumulative importance. Three models were evaluated as references: the full model with all 23 features, a reduced model with the top-\(k\) features, and an ablation model excluding microhomology features, using a tuned CatBoost classifier to assess feature contributions and overall classification performance.

\subsection{Cumulative Importance Curve}

The cumulative importance curve was computed using the tuned CatBoost classifier. Figure~\ref{fig:cumulative_importance} illustrates the contribution of features sorted by importance. The curve rises steeply for the first few features and then gradually plateaus, indicating that a small number of features capture most of the model's predictive power. A cumulative importance of 95\% is reached at \(k = 4\) features, which are \texttt{total\_clipped\_bases}, \texttt{kmer\_js\_divergence}, \texttt{kmer\_cosine\_diff}, and \texttt{softclip\_left}. 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{../notebooks/figures/catboost_cumulative_importance.png}
\caption{Cumulative importance curve of features sorted by importance.}
\label{fig:cumulative_importance}
\end{figure}

\subsection{Performance Comparison Across Feature Sets}

Classification performance was compared across three feature sets using a tuned CatBoost classifier. The full model, incorporating all 23 engineered features, achieved an F1 score of approximately 0.7686 and a ROC--AUC of 0.8436. A reduced model using only the top four features (\texttt{total\_clipped\_bases}, \texttt{kmer\_js\_divergence}, \texttt{kmer\_cosine\_diff}, and \texttt{softclip\_left}) achieved nearly equivalent performance with an F1 of 0.7670 and a ROC--AUC of 0.8353. An ablation model excluding microhomology features (\texttt{microhomology\_length} and \texttt{microhomology\_gc}) also performed comparably, with an F1 of 0.7679 and ROC--AUC of 0.8447. These results indicate that clipping and k-mer features capture almost all predictive signal, while microhomology features are largely redundant in this dataset.

\begin{table}[H]
\centering
\caption{Test set performance of three feature set variants using tuned CatBoost.}
\label{tab:catboost_feature_sets}
\begin{tabular}{lrrr}
\toprule
Variant & No. of Features & Test F1 & ROC--AUC \\
\midrule
Full CatBoost & 23 & 0.7686 & 0.8436 \\
Selected (top-4) & 4 & 0.7670 & 0.8353 \\
No microhomology & 21 & 0.7679 & 0.8447 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:catboost_three_sets} presents a bar chart comparing F1 and ROC--AUC across the three variants, with the x-axis showing the model variants and two bars per group representing the F1 and ROC--AUC values.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{../notebooks/figures/catboost_three_sets.png}
\caption{Comparison of F1 and ROC--AUC for the full, top-4 selected, and no-microhomology feature set variants.}
\label{fig:catboost_three_sets}
\end{figure}

\subsection{Interpretation and Final Feature Set Choice}

The full 23-feature model is retained as the primary configuration for the remainder of the study, while the four-feature subset serves as a lightweight alternative. Clipping features reflect alignment junctions and mapping disruptions typical of chimeric reads, and k-mer divergence captures changes in sequence composition across breakpoints. Microhomology features appear largely redundant, as their signal is either indirectly represented by clipping and k-mer features or not strongly expressed in the simulation dataset.


\section{Summary of Findings}

All models performed substantially better than the dummy baseline, with test F1-scores around 0.76 and ROC--AUC values near 0.84. Hyperparameter tuning yielded modest improvements, with boosting methods, particularly CatBoost and gradient boosting, achieving the highest performance. Confusion matrices and precision-recall curves indicate that the models prioritize precision over recall for chimeric reads, minimizing false positives.

Feature importance analysis highlighted alignment breakpoints, such as clipping, and abrupt shifts in k-mer composition as the main contributors to predictive power. Microhomology metrics and supplementary alignment features had minimal impact. These findings suggest that alignment-based and k-mer–based features alone are sufficient for training classifiers to detect mitochondrial PCR-induced chimeric reads under the conditions tested.